{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca000c-90ba-4ed4-89a6-97df9919373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef97aa-0983-4143-9d6c-28a03a58562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cf0f6-1ad1-4126-afb3-10d165ceaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import math, torch, matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from operator import attrgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from miniai.conv import *\n",
    "\n",
    "from fastprogress import progress_bar, master_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2dd64c-b109-45a1-8697-3a1b8e42c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import torchvision.transforms.functional as TF\n",
    "from contextlib import contextmanager\n",
    "from torch import nn, tensor\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "from miniai.datasets import *\n",
    "import logging\n",
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84264e5e-523e-42fa-a1e4-0cbc0543c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=160, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c8a55-2596-4872-9884-f6b118b84636",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b9019-2777-40ba-9169-23a074549e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed751e6-c462-4562-a961-b3bea46a3bd6",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e951d3-fb8f-4091-bba2-e9a45f19a849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc75a1f20f6d430a8496dbd8e15cadb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6917d24178449d89262343456914694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bdfc337bd047d1b7e40b461c39e482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2107ac7039b49699c7b86d385e0721c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b16e0fc4af40c3b65e08da17e8967f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/29.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede976c4722244e780c5003bd48d20ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915aee00877b4055873d7d5d5cd443ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d22f18f63f4fbd89716637698be542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba92f850a83d4b0fade79cdf79c3d036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = 'image', 'label'\n",
    "name = 'fashion_mnist'\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08188e7f-6ca1-4c44-94c3-1dc684deea3a",
   "metadata": {},
   "source": [
    "Grab a single example from Dataset and check its shape. We will be working with flattened images, so we convert PIL to tensor and flatten it out to get 784 (28x28) long tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc797c-5ade-43ed-a9ea-1eece904155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = TF.to_tensor(dsd['train'][0][x])\n",
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb8273-24b3-4207-8ee7-be2c229e0dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(TF.to_tensor(dsd['train'][0][x])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f03901-0bbb-4008-ae1e-088b946eeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b):\n",
    "    # b is a dictionary of image and label\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8902b-69d6-47ff-834a-dedcb58bb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "tds = dsd.with_transform(transformi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede909a-ae34-4899-94f0-d225749065a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 784]), tensor([5, 4, 9, 4, 3, 0, 6, 5, 7, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove number of workers during debugging\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=4)\n",
    "\n",
    "dt = dls.train\n",
    "xb, yb = next(iter(dt))\n",
    "xb.shape, yb[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728e8ca-eb4e-42c6-81a1-a172f03496ba",
   "metadata": {},
   "source": [
    "Let's remind ourselves how `DataLoaders.from_dd` actually works. \n",
    "\n",
    "```python\n",
    "def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n",
    "        f = collate_dict(dd['train'])\n",
    "        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f, **kwargs))\n",
    "```\n",
    "Let's first deal with the first line of code and `f` function. `f` is an internal collation function that itself returns a function returning collated `x` and `y` values from a given batch. This is how it is done:\n",
    "1. `collate_dict` takes feature names from `dd['train']`: `image` and `label` and puts them into an itemgetter\n",
    "2.  this itemgetter is applied to the result of a `default_collate` on a given Dataset returning a tuple of `x` and `y` values\n",
    "\n",
    "```python\n",
    "def collate_dict(ds):\n",
    "    get = itemgetter(*ds.features) # get x and y values\n",
    "    def _f(b): return get(default_collate(b))\n",
    "    return _f\n",
    "```\n",
    "\n",
    "The second line of code (returning) can be broken down into several parts: call `get_dls` and wrap its return value into `cls`. Let's tackle it one by one.\n",
    "\n",
    "```python\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, bs, shuffle=True, **kwargs), DataLoader(valid_ds, bs*2, **kwargs))\n",
    "```\n",
    "\n",
    "`get_dls` takes a train and valid datasets and turns them into the respective DataLoaders (PyTorch). Our created `collate_dict` function (assigned to `f`) is passed to the DataLoader `__init__` method via `**kwargs`\n",
    "\n",
    "Wrapping the resulting tuple of DataLoaders inside `cls` simply allows us to get them by calling .train and .vaild on our Dataloaders class.\n",
    "\n",
    "```python\n",
    "def __init__(self, *dls): self.train, self.valid = dls[:2]\n",
    "```\n",
    "\n",
    "Now we should fully understand the initial code above. We create dataloaders (train and valid), then we select train dataloader and get one batche of size 1024 from it.\n",
    "\n",
    "```python\n",
    "dls = DataLoaders.from_dd(tds, bs)\n",
    "dt = dls.train\n",
    "xb, yb = next(iter(dt))\n",
    "```\n",
    "During the call to iterator our collation function (`f`) comes in play and merges the 1024 element list of dictionaries into a tuple of `x` and `y` tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0395dc7-534c-423b-81ec-42b35be02a35",
   "metadata": {},
   "source": [
    "Now let's move on to creating our first Learner framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12dbc8-38e8-4c78-8a1d-ebe4d5d59692",
   "metadata": {},
   "source": [
    "##  Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3fdd3-e566-4a8e-b873-fe45afb9e9b0",
   "metadata": {},
   "source": [
    "First we create a simplified learner that still has some nice structure to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af106857-fc8c-4f83-8caa-50e976f5fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n",
    "    \n",
    "    def one_batch(self):\n",
    "        self.xb, self.yb = to_device(self.batch)\n",
    "        self.preds = self.model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad(): self.calc_stats()\n",
    "        \n",
    "    def calc_stats(self):\n",
    "        acc = (self.preds.argmax(dim=-1)==self.yb).float().sum()\n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss * n)\n",
    "        self.ns.append(n)\n",
    "          \n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for _, self.batch in enumerate(dl): self.one_batch()\n",
    "        n = sum(self.ns)\n",
    "        print(self.epoch, self.model.training, \n",
    "              sum(self.losses).item()/n, sum(self.accs).item()/n)\n",
    "        \n",
    "    def fit(self, n_epochs):\n",
    "        self.accs, self.losses, self.ns = [], [], []\n",
    "        self.model.to(def_device)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)\n",
    "            with torch.no_grad(): self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255723b-43bb-4f29-aa59-6f672e91862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, nh = 28*28, 50\n",
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c865a6b-d8f3-4fbb-a37a-e9409d213237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 1.17530390625 0.5987\n",
      "0 False 1.1203112723214286 0.6135857142857143\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54458635-6ddd-4990-bbfa-94bebc343daa",
   "metadata": {},
   "source": [
    "## Basic Callbacks Learner "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0057b-50e6-4fee-a1e3-8aa50de9619e",
   "metadata": {},
   "source": [
    "Now let's improve greatly upon our previus iteration and add Callbacks to our Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2803c9-5867-46df-acc0-730b582a15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148636c-5649-45b2-846f-ffb4dd7327fb",
   "metadata": {},
   "source": [
    "Note that we set order variable outside `__init__` and so it belongs to the class and is shared by all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb20c4f-bf71-48f5-9c16-5d91ce3fadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback: order = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873c7f3-89ad-4b53-b4c4-bc26b643b31b",
   "metadata": {},
   "source": [
    "We use `getattr` to get a named attribute from an object; getattr(x, 'y') is equivalent to x.y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660fe97-cfca-40f0-9cd8-2fb30cef6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        #?\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f0625-2f15-4524-99aa-30fa59b510a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionCB(Callback):\n",
    "    def before_fit(self, learn): self.count = 0\n",
    "    def after_batch(self, learn): self.count += 1\n",
    "    def after_fit(self, learn): print(f'Completed {self.count} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c51de-66d3-4937-a4bf-1f5e4f2bdc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 batches\n"
     ]
    }
   ],
   "source": [
    "cbs = [CompletionCB()]\n",
    "run_cbs(cbs, 'before_fit')\n",
    "run_cbs(cbs, 'after_batch')\n",
    "run_cbs(cbs, 'after_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59593a-3bf3-48d0-a819-3f0c10dedf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n",
    "    \n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        \n",
    "    def one_epoch(self, train: bool):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter, self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "            \n",
    "    def fit(self, n_epochs):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "            \n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb81d3-5024-4b6f-9653-cc044021ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,nh = 28*28, 50\n",
    "def get_model(): return nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f58861-0f3f-43b8-87e0-c3863a6f65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 64 batches\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[CompletionCB()])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d5de1-b54a-45d3-9df7-002586d5f679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9bd37-dce0-4687-ab98-9c3ae8f80646",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, bs, shuffle=True, **kwargs), DataLoader(valid_ds, bs*2, **kwargs))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2855e4c-cbd0-48cd-8db0-95f4cd7dfbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1024 * 2 because our dataloader for validation takes twice the batch size\n",
    "math.ceil(60_000/1024) + math.ceil(10_000/(1024*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11f161-df22-456a-bee4-4c93c2656201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SingleBatchCB(Callback):\n",
    "    order = 1\n",
    "    def after_batch(self, learn): raise CancelFitException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b427d8-418a-45d0-9a7d-4192f8ce8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(get_model(), dls, F.cross_entropy, lr=0.2, cbs=[SingleBatchCB(), CompletionCB()])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515a2d7-f989-43d4-9f21-2486ece2e493",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9a1ad-ea72-4b87-b6f0-fda2a91b71df",
   "metadata": {},
   "source": [
    "We want to be able to use different metrics with our learner so we create a base class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddf735-4b1e-4c9d-ad69-5709904a8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self): self.reset()\n",
    "    \n",
    "    def reset(self): self.vals, self.ns = [], []\n",
    "    \n",
    "    def add(self, inp, targ=None, n=1):\n",
    "        self.last = self.calc(inp, targ)\n",
    "        self.vals.append(self.last)\n",
    "        self.ns.append(n)\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        ns = tensor(self.ns)\n",
    "        # return weighted sum of metric\n",
    "        return (tensor(self.vals)*ns).sum()/ns.sum()\n",
    "        \n",
    "    def calc(self, inps, targs): return inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf7375-04e0-4fd8-9882-331096b52602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "    def calc(self, inps, targs): return (inps==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b681e4c-caba-4c54-97d3-ad3f817dfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.45)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = Accuracy()\n",
    "acc.add(tensor([0,1,2,0,1,2]),tensor([0,1,1,2,1,0]))\n",
    "acc.add(tensor([1,1,2,0,1]),tensor([0,1,1,2,1]))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c0f44-8b4a-4b38-84d6-52693ce79209",
   "metadata": {},
   "source": [
    "Basic Metric can serve as a weighted average calculator because its `calc` method simply return input back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e852a-9d35-4d26-bfdf-14a315dbb649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.62), 0.62)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = Metric()\n",
    "loss.add(0.6, n=32)\n",
    "loss.add(0.9, n=2)\n",
    "loss.value, round((0.6*32 + 0.9*2)/(32+2),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bf145-3f83-4303-a0d3-390790c7dec6",
   "metadata": {},
   "source": [
    "## Some callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d43fcd-e472-4540-8a37-a81d93821042",
   "metadata": {},
   "source": [
    "Let's build upon the Metric class and add some metrics defined in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e7de9-7ffb-4492-82e2-5973014c712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torcheval.metrics import MulticlassAccuracy, Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba4b96-f669-4127-bfe2-542b683877bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.50)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = MulticlassAccuracy()\n",
    "metric.update(tensor([0, 2, 1, 3]), tensor([0, 1, 2, 3]))\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20a25f-54f7-4a12-8a39-85a30d470138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.reset()\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8499851-ce65-4bae-adfd-8ebf185e2f5f",
   "metadata": {},
   "source": [
    "Metrics are usually calculated using data that is on the `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580150f-1189-4b5d-bae8-6d06505b3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def to_cpu(x):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    \"\"\"Recursively move data to cpu\"\"\"\n",
    "    # mapping case\n",
    "    if isinstance(x, Mapping): return {k: to_cpu(v) for k,v in x.items()}\n",
    "    # list case\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    # tuple case (via list)\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    # base case (recursive)\n",
    "    res = x.detach().cpu()\n",
    "    return res.float() if res.dtype == torch.float16 else res\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbb04e-a84d-4e11-9020-f143fbcf9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (tensor(1), tensor(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba6c71-4a57-41d7-9529-2b1a89d78899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), tensor(2))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cpu(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61580329-5267-4ed6-b417-851563134f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricsCB(Callback):\n",
    "    def __init__(self, \n",
    "                 *ms, # list of metrics\n",
    "                 **metrics, # dictionary of metrics\n",
    "                ):\n",
    "        # pool all unnamed metrics into a dictionary\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "        \n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n",
    "    \n",
    "    # print metrics after each epoch\n",
    "    def after_epoch(self, learn):\n",
    "        # save log of all metrics\n",
    "        log = {k: f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = learn.epoch\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        self._log(log)\n",
    "    \n",
    "    def after_batch(self,learn):\n",
    "        # unpack all remaining values into *\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe758a-5085-4e63-ace9-ff9c91711200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 1, 'str': '2', 'float': 3.0, 'list': [4]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MetricsCB(1,'2',3.0,[4])\n",
    "test.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b19f0-586b-4957-a8c6-e0b1ae4d924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def o(): return 1,2,3,4,5\n",
    "x1, x2, *_ = o()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005ddbc-960e-4a87-bd3f-afa4f68ffd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d9b39-fbd0-450e-8ade-1a2522dcec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '0.602', 'loss': '1.183', 'epoch': 0, 'train': 'train'}\n",
      "{'accuracy': '0.700', 'loss': '0.847', 'epoch': 0, 'train': 'eval'}\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB(), metrics])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71626a06-685b-4f36-9b4c-707d06152526",
   "metadata": {},
   "source": [
    "## Flexible learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5e6ee-e178-4d04-b98e-441b11cbf1c4",
   "metadata": {},
   "source": [
    "Let's continue imporving upon our Learner. Below we discuss the key concepts that we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8c4d8-091e-4657-822b-f9968de26d79",
   "metadata": {},
   "source": [
    "loss1. __Contexmanager__\n",
    "\n",
    "Context managers are objects that define the behavior to be performed when entering and exiting a context (e.g., acquiring and releasing resources). The contextmanager decorator simplifies the creation of context managers by eliminating the need to define a class with __enter__() and __exit__() methods.<br>\n",
    "\n",
    "Inside contextmanager we use `yield` without any value. `yield` expression returns control to the whatever is using the generator. The generator pauses at this point, which means that the `@contextmanager` decorator knows that the code is done with the setup part. In other words, everything you want to do in the context manager `__enter__` phase has to take place before the yield.\n",
    "\n",
    "Once the context exits (so the block under the with statement is done), the `@contextmanager` decorator is called for the `__exit__` part of the context manager protocol and will do one of two things:\n",
    "\n",
    "* If there was no exception resume your generator. The generator unpauses at the yield line, and enter the cleanup phase\n",
    "* If there was an exception, the decorator uses generator.throw() to raise that exception in the generator. It'll be as if the yield line caused that exception. Because we have a `finally` clause, it'll be executed before your generator exits because of the exception.\n",
    "\n",
    "In our example the sequence is as follows:\n",
    "\n",
    "1. with self.cb_ctx('epoch') creates the context manager and calls `__enter__` on that. \n",
    "\n",
    "2. The generator starts execution and tries calling `self.callback(f'before_{nm}')`\n",
    "\n",
    "3. The `yield` expression pauses the generator, control goes back to the decorator. This takes whatever was yielded and returns that to the `with` statement, in case there is an `as target` part. Here `None` is yielded (there is only a plain `yield` expression).\n",
    "\n",
    "4. Code below `with` statement:\n",
    "```python\n",
    "for self.iter, self.batch in enumerate(self.dl):\n",
    "```\n",
    "is run and completes.\n",
    "\n",
    "5. The context manager __exit__ method is run, no exception is passed in.\n",
    "\n",
    "6. The decorator resumes the generator, it continues where it left off.\n",
    "\n",
    "7. The `finally` block is entered and `self.callback(f'cleanup_{nm}')` is executed\n",
    "\n",
    "8. The generator exits, the decorator __exit__ method exits, all is done.\n",
    "\n",
    "\n",
    "See https://stackoverflow.com/questions/35489844/what-does-yield-without-value-do-in-context-manager/35489897#35489897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa13ef-43bd-4df8-ab51-2d3971551665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        # why do we need it? Does it affect fc.store_attr? It seems that is has no effect\n",
    "        # cb = fc.L(cbs)\n",
    "        fc.store_attr()\n",
    "    \n",
    "    @contextmanager\n",
    "    def cb_ctx(self, nm):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        try:\n",
    "            self.callback(f'before_{nm}')\n",
    "            yield\n",
    "            self.callback(f'after_{nm}')\n",
    "        except globals()[f'Cancel{nm.title()}Exception']: pass\n",
    "        finally: self.callback(f'cleanup_{nm}')\n",
    "            \n",
    "    \n",
    "    def one_epoch(self, train: bool):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        # set the model to the right mode and select relevant dataloader\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        # enter context for an epoch\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        with self.cb_ctx('epoch'):\n",
    "            # loop through batches\n",
    "            for self.iter, self.batch in enumerate(self.dl):\n",
    "                # enter context for a batch\n",
    "                with self.cb_ctx('batch'):\n",
    "                    self.predict()\n",
    "                    self.get_loss()\n",
    "                    if self.training:\n",
    "                        self.backward()\n",
    "                        self.step()\n",
    "                        self.zero_grad()\n",
    "\n",
    "    \n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        # cast cbs to L\n",
    "        cbs = fc.L(cbs)\n",
    "        # if additional cbs are provided, pass it to the Learner for fitting and remove later\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        # try block\n",
    "        try:\n",
    "            # create number of epochs, their range and optimizer\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n",
    "            # fit context\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            with self.cb_ctx('fit'):\n",
    "                # train and validate for a given number of epochs\n",
    "                for self.epoch in self.epochs:\n",
    "                    if train: self.one_epoch(True)\n",
    "                    # acts like a context manager with grad calculation disabled\n",
    "                    if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "        finally:\n",
    "            # remove cbs\n",
    "             for cb in cbs: self.cbs.remove(cb)\n",
    "      \n",
    "    # __getattr__ method to call predict, get_loss, backward, step and zero_grad directly on Learner\n",
    "    def __getattr__(self, name):\n",
    "        if name in (\"predict\", \"get_loss\", \"backward\", \"step\", \"zero_grad\"): \n",
    "            # partial (instead of self.callback(name)) because we want to return a function, not a value (which is None)\n",
    "            # run callback method that loops through all callbacks and calls those that have a specified name. For example `predict` from TrainCB\n",
    "            return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "       \n",
    "    # run callback on a Learner\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    # for ease of reference\n",
    "    @property\n",
    "    def training(self): return self.model.training                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccecd6-2019-4a01-858f-cc2e6f59305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainCB(Callback):\n",
    "    \"\"\"Basic training callback\"\"\"\n",
    "    # n_inp allows to train models with more than one input\n",
    "    def __init__(self, n_inp=1): self.n_inp = n_inp\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bf662-5f26-4d5a-af0e-6572401e6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressCB(Callback):\n",
    "    # decrease callback priority\n",
    "    order = MetricsCB.order + 1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        # create master_bar and set it to both mbar and learn.epochs\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        # substitute _log method of learn's metrics (simple print) with progress bar\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "        \n",
    "    def before_epoch(self, learn): \n",
    "        # ??\n",
    "        learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "        \n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            if self.val_losses: \n",
    "                self.mbar.update_graph(\n",
    "                    [[fc.L.range(self.losses), self.losses],\n",
    "                     [fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), \n",
    "                      self.val_losses]])\n",
    "                   \n",
    "    def after_epoch(self, learn):\n",
    "        if not learn.training:\n",
    "            if self.plot and hasattr(learn, 'metrics'):\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                # append to validation losses\n",
    "                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n",
    "                self.mbar.update_graph(\n",
    "                    # plot training losses\n",
    "                    [[fc.L.range(self.losses), self.losses],\n",
    "                     # plot validation losses, converting from epochs to batches\n",
    "                     [fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), \n",
    "                      self.val_losses]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225312c-7548-4cd8-8207-a2f636d28508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4692e-bf97-4d13-a64d-5c845b11a16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.600</td>\n",
       "      <td>1.179</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:02&lt;00:00 0.767]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.9/bdb.py\", line 334, in set_trace\n",
      "    sys.settrace(self.trace_dispatch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_90/1864303131.py\u001b[0m(41)\u001b[0;36mafter_epoch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     40 \u001b[0;31m                \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 41 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     42 \u001b[0;31m                self.mbar.update_graph(\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m(24)\u001b[0;36mdecorate_context\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     23 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 24 \u001b[0;31m        \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m(26)\u001b[0;36mdecorate_context\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m(27)\u001b[0;36mdecorate_context\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     26 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "tensor(0.78, ...torch.float64)\n",
      "> \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m(27)\u001b[0;36mdecorate_context\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     26 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_90/1864303131.py\u001b[0m(42)\u001b[0;36mafter_epoch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     41 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m                self.mbar.update_graph(\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m                    [[fc.L.range(self.losses), self.losses],\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.9/bdb.py\", line 359, in set_quit\n",
      "    sys.settrace(None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3852e-a00c-4a20-ab76-05e9cb178063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
