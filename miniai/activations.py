# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_activations.ipynb.

# %% ../nbs/10_activations.ipynb 3
from __future__ import annotations
import random, math, torch, numpy as np, matplotlib.pyplot as plt
import fastcore.all as fc
from functools import partial

from .datasets import *
from .learner import *

# %% auto 0
__all__ = ['set_seed', 'Hook', 'Hooks', 'HooksCallback', 'append_stats', 'get_hist', 'get_min', 'ActivationStats']

# %% ../nbs/10_activations.ipynb 6
def set_seed(seed, determenistic=False):
    # make torch use deterministic algorithms when available
    torch.use_deterministic_algorithms(determenistic)
    # set seed for torch, random and numpy
    torch.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)

# %% ../nbs/10_activations.ipynb 39
class Hook:
    def __init__(self, 
                 m, # module that we a hooking
                 f # function
                ):
        # register hook with a function on a layer (module)
        self.hook = m.register_forward_hook(partial(f, self))
    # remove hook
    def remove(self): self.hook.remove()
    # del method
    def __del__(self): self.remove()

# %% ../nbs/10_activations.ipynb 57
class Hooks(list):
    # register a Hook for each module using class constructor
    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])
    # enter context
    def __enter__(self, *args): return self
    # exit context
    def __exit__(self, *args): self.remove()
    # del Hooks
    def __del__(self): self.remove()
    # del separate item
    def __delitem__(self, i):
        self[i].remove()
        super().__delitem__(i)
    # `remove` called by del
    def remove(self):
        for h in self: h.remove()

# %% ../nbs/10_activations.ipynb 62
class HooksCallback(Callback):
    def __init__(self, 
                 hookfunc, # hook function
                 mod_filter=fc.noop, # filter modules to hook to
                 on_train=True, # use hooks on train
                 on_valid=False, # use hooks on valid
                 mods=None): # modules to hook to
        fc.store_attr()
        super().__init__()
    
    
    def before_fit(self, learn):
        # if modules are passed - use them
        if self.mods: mods = self.mods
        # if no modules are passed - grab all modules from model and apply filter to it from fastcore
        else: mods = fc.filter_ex(learn.model.modules(), self.mod_filter)
        # hook onto modules with a _hookfunc wrapper
        self.hooks = Hooks(mods, partial(self._hookfunc, learn))
        
    
    def _hookfunc(self, learn, *args, **kwargs):
        # check that if our hook is applied during training - our model is training and
        # vice versa for validation
        if (self.on_train and learn.training) or (self.on_valid and not learn.training):
            self.hookfunc(*args, **kwargs)

    # remove hooks after fit
    def after_fit(self, learn): self.hooks.remove()
    
    # make calss an iterable
    def __iter__(self): return iter(self.hooks)

    # get length
    def __len__(self): return len(self.hooks)

# %% ../nbs/10_activations.ipynb 69
def append_stats(hook, mod, inp, outp):
    # if no stats attribute - create it
    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
    # move outputs to cpu
    acts = to_cpu(outp)
    # add activation means, std and histogram of absolute values (40 bins, min=0 to max=10)
    hook.stats[0].append(acts.mean())
    hook.stats[1].append(acts.std())
    hook.stats[2].append(acts.abs().histc(40,0,10))

# %% ../nbs/10_activations.ipynb 76
# Thanks to @ste for initial version of histgram plotting code
# stack stats together in a single tensor, transpose it, cast to float and log1p
def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()

# %% ../nbs/10_activations.ipynb 80
def get_min(h):
    # get transposed tensor as in `get_hist`
    h1 = torch.stack(h.stats[2]).t().float()
    # take ratio of lowest bin (near zero) and check their ratio to the rest acivations
    # do this for each batch
    return h1[0] / h1.sum(0)

# %% ../nbs/10_activations.ipynb 86
class ActivationStats(HooksCallback):
    # super init
    def __init__(self, mod_filter=fc.noop): super().__init__(append_stats, mod_filter)
    
    
    def color_dim(self, figsize=(11,5)):
        # grab grid based on length of stats
        fig, axes = get_grid(len(self), figsize=figsize)
        # plot colorful dims plots for each hook
        for ax, h in zip(axes.flat, self):
            show_image(get_hist(h), ax, origin='lower')
      
    
    def dead_chart(self, figsize=(11,5)):
        # grab grid based on length of stats
        fig, axes = get_grid(len(self), figsize=figsize)
        # plot dead_chart (min(h)
        for ax, h in zip(axes.flat, self):
            ax.plot(get_min(h))
            ax.set_ylim(0,1)
    
    
    def plot_stats(self, figsize=(10,4)):
        # create subplots and plot means and std
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        for h in self:
            for i in 0,1: axes[i].plot(h.stats[i])
        axes[0].set_title('Means')
        axes[1].set_title('Stdevs')
        plt.legend(fc.L.range(self))
